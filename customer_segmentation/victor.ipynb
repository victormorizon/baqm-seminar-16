{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gower\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, AgglomerativeClustering, HDBSCAN, Birch, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "from itertools import combinations, chain\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import kmedoids\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "segments = pd.read_csv(\"./segments.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "\n",
    "df = df[df[\"first_data_year\"] >= 2021].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_init = [\"welcome_discount\", \"policy_nr_hashed\", \"control_group\", \"churn\", \"last_data_year\", \"first_datapoint_year\", \"last_datapoint_year\", \"first_data_year\", 'last_type', 'lpa', 'count', 'cluster']\n",
    "cols_to_keep = [col for col in df.columns if col not in cols_to_drop_init]\n",
    "\n",
    "df_filt = df[cols_to_keep]\n",
    "df_filt_preapplied = pd.merge(df[cols_to_keep + [\"policy_nr_hashed\"]], segments, on='policy_nr_hashed', how='inner').drop(\"policy_nr_hashed\", axis=1)\n",
    "\n",
    "dist_matrix = gower.gower_matrix(df_filt)\n",
    "# dist_matrix = pd.read_csv(\"../data/gower_matrix.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(dist_matrix, n):\n",
    "    cluster = MiniBatchKMeans(n_clusters=n, random_state=0, n_init='auto').fit(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster.labels_)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster.labels_)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster.labels_)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster\n",
    "\n",
    "def kmedoids_cluster(dist_matrix, n):\n",
    "    cluster = kmedoids.KMedoids(n, method='fasterpam', init='build', random_state=0).fit(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster.labels_)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster.labels_)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster.labels_)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster\n",
    "\n",
    "def spectral_cluster(dist_matrix, n):\n",
    "    cluster_labels = SpectralClustering(n_clusters=n, n_init=100, assign_labels='discretize', affinity=\"precomputed\").fit_predict(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster_labels)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster_labels)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster_labels)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster_labels\n",
    "\n",
    "def hiererchichal_cluster(dist_matrix, n):\n",
    "    cluster_labels = AgglomerativeClustering(n_clusters=n, linkage='complete', metric=\"precomputed\").fit_predict(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster_labels)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster_labels)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster_labels)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh_score, db_score, ch_score, cluster = kmeans_cluster(dist_matrix, 5)\n",
    "sh_score, db_score, ch_score, cluster = kmedoids_cluster(dist_matrix, 4)\n",
    "# sh_score, db_score, ch_score, cluster = spectral_cluster(dist_matrix, 5)\n",
    "\n",
    "print(f\"Silhouette Score: {np.round(sh_score, 3)}\")\n",
    "print(f\"Davies Bouldin: {np.round(db_score, 3)}\")\n",
    "print(f\"Calinski Harabasz Score: {np.round(ch_score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_list = []\n",
    "db_list = []\n",
    "ch_list = []\n",
    "n_list = []\n",
    "\n",
    "for n in tqdm(np.arange(2, 11, 1)):\n",
    "    sh_score, db_score, ch_score, cluster = hiererchichal_cluster(dist_matrix, int(n))\n",
    "\n",
    "    sh_list.append(sh_score)\n",
    "    db_list.append(db_score)\n",
    "    ch_list.append(ch_score)\n",
    "    n_list.append(n)\n",
    "\n",
    "print(np.max(sh_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(n_list, ch_list, marker ='.', color='cornflowerblue')\n",
    "# plt.xticks(n_list)\n",
    "# plt.xlabel(\"Number of Clusters\")\n",
    "# plt.ylabel(\"Silhouette Score\")\n",
    "# plt.axvline(4, linestyle='--', color='coral', label='Constraint')\n",
    "# plt.savefig('../plots/segments_sh.png', dpi=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filt_preapplied.groupby(\"cluster\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh_score, db_score, ch_score, cluster = kmedoids_cluster(dist_matrix, 5)\n",
    "\n",
    "mean_agg = {col: pd.NamedAgg(column=col, aggfunc='mean') for col in df_filt.columns if df_filt[col].dtype != 'object'}\n",
    "\n",
    "df_clust = (\n",
    "    df_filt_preapplied\n",
    "    .groupby(\"cluster\")\n",
    "    .agg(\n",
    "        **mean_agg\n",
    "    )\n",
    ").drop([\"last_postcode\", \"perc_western_ppl\", \"perc_nld_ppl\", \"perc_others_ppl\", \"last_allrisk royaal\", \"last_allrisk compleet\", \"last_vs_first_split\", \"last_wa-extra\", \"policyholder_change\", \"n_last_vs_peak\", \"fake_alarm\", \"last_allrisk basis\", \"last_split\", \"max_nr_coverages\", \"nr_years\", \"cum_change_premium_abs\", \"cum_change_premium_perc\", \"pc4\", \"last_premium\"], axis=1)\n",
    "\n",
    "np_clust_expl = StandardScaler().fit_transform(df_clust)\n",
    "\n",
    "def top_5_columns_with_values(row):\n",
    "    # Get the top 5 absolute values and their corresponding column names\n",
    "    top_5 = row.abs().nlargest(10)\n",
    "    # Create a dictionary mapping column names to their raw values in the row\n",
    "    top_5_dict = {col: np.round(row[col], 2) for col in top_5.index if np.abs(row[col]) >= 0.8}\n",
    "    return top_5_dict\n",
    "\n",
    "df_clust_expl = pd.DataFrame(np_clust_expl, columns=df_clust.columns, index=df_clust.index)\n",
    "\n",
    "top_5_per_row = df_clust_expl.apply(top_5_columns_with_values, axis=1).tolist()\n",
    "\n",
    "for i in range(len(top_5_per_row)):\n",
    "    print(i, \":\", top_5_per_row[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filt[[col for col in df_filt.columns if col != \"cluster\"]]\n",
    "y = df_filt['cluster']\n",
    "\n",
    "for col in X.columns:\n",
    "     if X[col].dtype == \"object\":\n",
    "        X[col] = X[col].astype(\"category\")\n",
    "\n",
    "# Run model selection\n",
    "space = {\n",
    "    'max_depth': hp.uniformint('max_depth', 50, 100),\n",
    "    'n_estimators': hp.uniformint('n_estimators', 50, 200),\n",
    "    'num_leaves': hp.uniformint('num_leaves', 2, 200),\n",
    "    'min_child_samples': hp.uniformint('min_child_samples', 7, 100),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.25, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.25, 1),\n",
    "    'subsample_freq': hp.uniformint('subsample_freq', 1, 100),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.2),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 0.2),\n",
    "    'min_split_gain': hp.uniform('min_split_gain', 0, 0.5),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    'min_data_in_leaf': hp.uniformint('min_data_in_leaf', 1, 21),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        force_row_wise=True,\n",
    "        verbosity=-1,\n",
    "        random_state=0,\n",
    "        # is_unbalance=True,\n",
    "        **params\n",
    "    )\n",
    "    score = cross_val_score(clf, X, y, cv=5, scoring=\"f1_macro\").mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "n_iter = 10\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=n_iter, trials=trials, rstate=np.random.default_rng(seed=0))\n",
    "\n",
    "print(\"Best Score is: \", -trials.best_trial['result']['loss'])\n",
    "print(\"Best Parameters: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_params = {\n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'n_estimators': int(best['n_estimators']),\n",
    "    'num_leaves': int(best['num_leaves']),\n",
    "    'min_child_samples': int(best['min_child_samples']),\n",
    "    'colsample_bytree': best['colsample_bytree'],\n",
    "    'subsample': best['subsample'],\n",
    "    'subsample_freq': int(best['subsample_freq']),\n",
    "    'reg_alpha': best['reg_alpha'],\n",
    "    'reg_lambda': best['reg_lambda'],\n",
    "    'min_split_gain': best['min_split_gain'],\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'min_data_in_leaf': int(best['min_data_in_leaf']),\n",
    "}\n",
    "\n",
    "lgbm_best = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    force_row_wise=True,\n",
    "    verbosity=-1,\n",
    "    random_state=0,\n",
    "    # is_unbalance=True,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {np.mean(cross_val_score(lgbm_best, X, y, cv=5, scoring='accuracy'))}\")\n",
    "print(f\"F1 Macro: {np.mean(cross_val_score(lgbm_best, X, y, cv=5, scoring='f1_macro'))}\")\n",
    "print(f\"F1 Micro: {np.mean(cross_val_score(lgbm_best, X, y, cv=5, scoring='f1_micro'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "lgbm_best = lgbm_best.fit(X_train, y_train)\n",
    "y_pred = lgbm_best.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
