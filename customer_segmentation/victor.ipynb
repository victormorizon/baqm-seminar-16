{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gower\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, AgglomerativeClustering, HDBSCAN, Birch, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import kmedoids\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "segments = pd.read_csv(\"./segments.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "\n",
    "# Head is to prevent long run times but if you want to generate the full data, you have to remove it and wait 45 minutes\n",
    "df = df[df[\"first_data_year\"] >= 2021].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_init = [\"welcome_discount\", \"policy_nr_hashed\", \"control_group\", \"churn\", \"last_data_year\", \"first_datapoint_year\", \"last_datapoint_year\", \"first_data_year\", 'last_type', 'lpa', 'count', 'cluster']\n",
    "cols_to_keep = [col for col in df.columns if col not in cols_to_drop_init]\n",
    "\n",
    "df_filt = df[cols_to_keep]\n",
    "df_filt_preapplied = pd.merge(df[cols_to_keep + [\"policy_nr_hashed\"]], segments, on='policy_nr_hashed', how='inner').drop(\"policy_nr_hashed\", axis=1)\n",
    "\n",
    "dist_matrix = gower.gower_matrix(df_filt)\n",
    "# dist_matrix = pd.read_csv(\"../data/gower_matrix.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(dist_matrix, n):\n",
    "    cluster = MiniBatchKMeans(n_clusters=n, random_state=0, n_init='auto').fit(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster.labels_)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster.labels_)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster.labels_)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster\n",
    "\n",
    "def kmedoids_cluster(dist_matrix, n):\n",
    "    cluster = kmedoids.KMedoids(n, method='fasterpam', init='build', random_state=0).fit(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster.labels_)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster.labels_)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster.labels_)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster\n",
    "\n",
    "def spectral_cluster(dist_matrix, n):\n",
    "    cluster_labels = SpectralClustering(n_clusters=n, n_init=100, assign_labels='discretize', affinity=\"precomputed\").fit_predict(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster_labels)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster_labels)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster_labels)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster_labels\n",
    "\n",
    "def hiererchichal_cluster(dist_matrix, n):\n",
    "    cluster_labels = AgglomerativeClustering(n_clusters=n, linkage='complete', metric=\"precomputed\").fit_predict(dist_matrix)\n",
    "\n",
    "    sh_score = silhouette_score(dist_matrix, cluster_labels)\n",
    "    db_score = davies_bouldin_score(dist_matrix, cluster_labels)\n",
    "    ch_score = calinski_harabasz_score(dist_matrix, cluster_labels)\n",
    "\n",
    "    return sh_score, db_score, ch_score, cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh_score, db_score, ch_score, cluster = kmeans_cluster(dist_matrix, 5)\n",
    "sh_score, db_score, ch_score, cluster = kmedoids_cluster(dist_matrix, 4)\n",
    "# sh_score, db_score, ch_score, cluster = spectral_cluster(dist_matrix, 5)\n",
    "\n",
    "print(f\"Silhouette Score: {np.round(sh_score, 3)}\")\n",
    "print(f\"Davies Bouldin: {np.round(db_score, 3)}\")\n",
    "print(f\"Calinski Harabasz Score: {np.round(ch_score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the data for the graphs to select the best nr of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_list = []\n",
    "db_list = []\n",
    "ch_list = []\n",
    "n_list = []\n",
    "\n",
    "for n in tqdm(np.arange(2, 11, 1)):\n",
    "    sh_score, db_score, ch_score, cluster = kmedoids_cluster(dist_matrix, int(n))\n",
    "\n",
    "    sh_list.append(sh_score)\n",
    "    db_list.append(db_score)\n",
    "    ch_list.append(ch_score)\n",
    "    n_list.append(n)\n",
    "\n",
    "print(np.max(sh_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(n_list, ch_list, marker ='.', color='cornflowerblue')\n",
    "# plt.xticks(n_list)\n",
    "# plt.xlabel(\"Number of Clusters\")\n",
    "# plt.ylabel(\"Silhouette Score\")\n",
    "# plt.axvline(4, linestyle='--', color='coral', label='Constraint')\n",
    "# plt.savefig('../plots/segments_sh.png', dpi=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of customers per clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filt_preapplied.groupby(\"cluster\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the characteristics of each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_agg = {col: pd.NamedAgg(column=col, aggfunc='mean') for col in df_filt.columns if df_filt[col].dtype != 'object'}\n",
    "\n",
    "df_clust = (\n",
    "    df_filt_preapplied\n",
    "    .groupby(\"cluster\")\n",
    "    .agg(\n",
    "        **mean_agg\n",
    "    )\n",
    ").drop([\"last_postcode\", \"perc_western_ppl\", \"perc_nld_ppl\", \"perc_others_ppl\", \"last_allrisk royaal\", \"last_allrisk compleet\", \"last_vs_first_split\", \"last_wa-extra\", \"policyholder_change\", \"n_last_vs_peak\", \"fake_alarm\", \"last_allrisk basis\", \"last_split\", \"max_nr_coverages\", \"nr_years\", \"cum_change_premium_abs\", \"cum_change_premium_perc\", \"pc4\", \"last_premium\"], axis=1)\n",
    "\n",
    "np_clust_expl = StandardScaler().fit_transform(df_clust)\n",
    "\n",
    "def top_5_columns_with_values(row):\n",
    "    # Get the top 5 absolute values and their corresponding column names\n",
    "    top_5 = row.abs().nlargest(10)\n",
    "    # Create a dictionary mapping column names to their raw values in the row\n",
    "    top_5_dict = {col: np.round(row[col], 2) for col in top_5.index if np.abs(row[col]) >= 0.8}\n",
    "    return top_5_dict\n",
    "\n",
    "df_clust_expl = pd.DataFrame(np_clust_expl, columns=df_clust.columns, index=df_clust.index)\n",
    "\n",
    "top_5_per_row = df_clust_expl.apply(top_5_columns_with_values, axis=1).tolist()\n",
    "\n",
    "for i in range(len(top_5_per_row)):\n",
    "    print(i, \":\", top_5_per_row[i])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
