{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, average_precision_score, make_scorer, precision_recall_curve, roc_curve, mean_absolute_error, median_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "segments = pd.read_csv(\"../customer_segmentation/segments.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = [col for col, val in df.isnull().any().to_dict().items() if val == True]\n",
    "print(nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = []\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "\n",
    "# Define a threshold for the maximum number of unique values for a categorical column\n",
    "max_unique_values_for_categorical = 5\n",
    "\n",
    "# Iterate through each column to determine if it's categorical, continuous, or binary\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    if set(df[column].unique()) == {0, 1}:\n",
    "        # If exactly 2 unique values, treat column as binary\n",
    "        binary_features.append(column)\n",
    "    elif (df[column].dtype == 'object' or unique_values <= max_unique_values_for_categorical) and unique_values > 2:\n",
    "        # If object type or up to the threshold of unique values (and more than 2), treat as categorical\n",
    "        categorical_features.append(column)\n",
    "    else:\n",
    "        # Otherwise, treat as continuous\n",
    "        continuous_features.append(column)\n",
    "\n",
    "categorical_features = [col for col in categorical_features if col != \"nr_years\"] + ['last_product']\n",
    "continuous_features = [col for col in continuous_features if col != \"last_product\"] + ['nr_years']\n",
    "\n",
    "print(f'Binary Features: {binary_features}')\n",
    "print(f'Categorical Features: {categorical_features}')\n",
    "print(f'Continuous Features: {continuous_features}')\n",
    "\n",
    "for col in categorical_features:\n",
    "     df[col] = df[col].astype(\"category\")\n",
    "\n",
    "for col in binary_features:\n",
    "     df[col] = df[col].astype(\"int\")\n",
    "\n",
    "for col in continuous_features:\n",
    "     df[col] = df[col].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop_manual = [\"first_split\", \"first_premium\", \"nr_cars\", \"last_type\", \"last_brand\", 'last_weight', 'n_last_vs_peak', 'last_fuel_type', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'max_nr_coverages', 'last_nr_coverages',]\n",
    "cols_to_drop = [\"churn\", \"policy_nr_hashed\", \"last_data_year\", \"first_data_year\", \"welcome_discount\", \"control_group\", 'count', 'first_datapoint_year', 'last_datapoint_year']\n",
    "selected_columns = [col for col in df.columns if not any(col.startswith(prefix) for prefix in cols_to_drop)]\n",
    "\n",
    "X = df[selected_columns]\n",
    "y = df['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'TP': 0,  # High importance to correctly identify churners\n",
    "#     'FP': 0,  # Moderate to high cost of misclassifying loyal customers\n",
    "#     'FN': 100.0,   # High cost of missing actual churners\n",
    "#     'TN': 100.0\n",
    "# }\n",
    "\n",
    "# def weighted_cost(y_true, y_scores, weights):\n",
    "\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_true, y_scores).ravel()\n",
    "\n",
    "#     # Weight everyone\n",
    "#     TP_wght = tp * weights['TP']\n",
    "#     FP_wght = fp * weights['FP']\n",
    "#     FN_wght = fn * weights['FN']\n",
    "#     TN_wght = tn * weights['TN']\n",
    "\n",
    "#     # Compute weighted cost and normalise\n",
    "#     wghtd_cost = TP_wght + FP_wght + FN_wght + TN_wght\n",
    "\n",
    "#     return -wghtd_cost\n",
    "\n",
    "# weighted_cost_scorer = make_scorer(weighted_cost, needs_proba=True, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'max_depth': hp.uniformint('max_depth', 50, 100),\n",
    "    'n_estimators': hp.uniformint('n_estimators', 50, 200),\n",
    "    'num_leaves': hp.uniformint('num_leaves', 2, 200),\n",
    "    'min_child_samples': hp.uniformint('min_child_samples', 7, 100),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.25, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.25, 1),\n",
    "    'subsample_freq': hp.uniformint('subsample_freq', 1, 100),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.2),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 0.2),\n",
    "    'min_split_gain': hp.uniform('min_split_gain', 0, 0.5),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    'min_data_in_leaf': hp.uniformint('min_data_in_leaf', 1, 21),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        force_row_wise=True,\n",
    "        verbosity=-1,\n",
    "        # n_estimators=50,\n",
    "        **params\n",
    "    )\n",
    "    score = cross_val_score(clf, X_train[X_train[\"welcome_discount\"] == 0], y_train, cv=5, scoring=\"neg_brier_score\").mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "n_iter = 50\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=n_iter, trials=trials)\n",
    "\n",
    "print(\"Best Score is: \", -trials.best_trial['result']['loss'])\n",
    "print(\"Best Parameters: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'colsample_bytree': 0.8415229383909493, 'learning_rate': 0.023375272621823318, 'max_depth': 73.0, 'min_child_samples': 49.0, 'min_data_in_leaf': 6.0, 'min_split_gain': 0.3349142609935463, 'n_estimators': 164.0, 'num_leaves': 180.0, 'reg_alpha': 0.07871752728134146, 'reg_lambda': 0.047610247070413274, 'subsample': 0.743174247931145, 'subsample_freq': 21.0}\n",
    "\n",
    "best_params = {\n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'n_estimators': int(best['n_estimators']),\n",
    "    'num_leaves': int(best['num_leaves']),\n",
    "    'min_child_samples': int(best['min_child_samples']),\n",
    "    'colsample_bytree': best['colsample_bytree'],\n",
    "    'subsample': best['subsample'],\n",
    "    'subsample_freq': int(best['subsample_freq']),\n",
    "    'reg_alpha': best['reg_alpha'],\n",
    "    'reg_lambda': best['reg_lambda'],\n",
    "    'min_split_gain': best['min_split_gain'],\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'min_data_in_leaf': int(best['min_data_in_leaf'])\n",
    "}\n",
    "\n",
    "lgbm_best = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    force_row_wise=True,\n",
    "    verbosity=-1,\n",
    "    **best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_prob(y_true, y_pred_probs):\n",
    "    return mean_absolute_error(y_true, y_pred_probs)\n",
    "\n",
    "def medae_prob(y_true, y_pred_probs):\n",
    "    return median_absolute_error(y_true, y_pred_probs)\n",
    "\n",
    "mae_prob_scorer = make_scorer(mae_prob, needs_proba=True)\n",
    "medae_prob_scorer = make_scorer(medae_prob, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_brier = cross_val_score(lgbm_best, X, y, cv=5, scoring='neg_brier_score')\n",
    "scores_log_loss = cross_val_score(lgbm_best, X, y, cv=5, scoring='neg_log_loss')\n",
    "scores_mae = cross_val_score(lgbm_best, X, y, cv=5, scoring=mae_prob_scorer)\n",
    "scores_medae = cross_val_score(lgbm_best, X, y, cv=5, scoring=medae_prob_scorer)\n",
    "\n",
    "print('CV Average Brier score: {0:0.4f}'.format(-np.mean(scores_brier)))\n",
    "print('CV Average Log Loss: {0:0.4f}'.format(-np.mean(scores_log_loss)))\n",
    "print('CV Average Root Brier score: {0:0.4f}'.format(np.sqrt(-np.mean(scores_brier))))\n",
    "print('CV MAE: {0:0.4f}'.format(np.mean(scores_mae)))\n",
    "print('CV MedAE: {0:0.4f}'.format(np.mean(scores_medae)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df[df[\"welcome_discount\"] == 0]\n",
    "\n",
    "X = df_filt[selected_columns]\n",
    "y = df_filt['churn']\n",
    "\n",
    "lgbm_best.fit(X, y)\n",
    "predictions = lgbm_best.predict_proba(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_np = np.array([i[1] for i in predictions])\n",
    "df['proba'] = predictions_np\n",
    "\n",
    "predictions_final = df.merge(segments, how=\"inner\", on=\"policy_nr_hashed\")\n",
    "\n",
    "predictions_final = predictions_final[predictions_final[\"welcome_discount\"] > 0]\n",
    "# predictions_final = predictions_final[predictions_final[\"welcome_discount\"] <= 0.3]\n",
    "\n",
    "display(predictions_final.groupby(\"cluster\").agg({\"proba\": \"mean\", \"churn\": \"mean\", \"welcome_discount\": \"mean\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = [4.1, 0.3, 3.1]\n",
    "group_2 = [1.6, 0.3, -3.8]\n",
    "group_3 = [3.8, 1, -0.6]\n",
    "group_4 = [3.1, -1.8, -0.4]\n",
    "\n",
    "x = [\"0%-16.2%\", \"16.2%-24.3%\", \"24.3%-30%\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(x,y)\n",
    "# ax.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "\n",
    "ax.plot(x, group_1, label=\"Value Seekers (1)\", marker=\"x\", linestyle='--', linewidth=1, color=\"firebrick\")\n",
    "# ax.fill_between(x, (group_1-group_1_ci), (group_1+group_1_ci), alpha=.1, color=\"firebrick\")\n",
    "\n",
    "ax.plot(x, group_2, label=\"High-Income Customers (2)\", marker=\"x\", linestyle='--', linewidth=1, color=\"goldenrod\")\n",
    "# ax.fill_between(x, (group_2-group_2_ci), (group_2+group_2_ci), alpha=.1, color=\"goldenrod\")\n",
    "\n",
    "ax.plot(x, group_3, label=\"Basic Coverage (3)\", marker=\"x\", linestyle='--', linewidth=1, color=\"darkcyan\")\n",
    "# ax.fill_between(x, (group_2-group_2_ci), (group_2+group_2_ci), alpha=.1, color=\"goldenrod\")\n",
    "\n",
    "ax.plot(x, group_4, label=\"Rural Customers (4)\", marker=\"x\", linestyle='--', linewidth=1, color=\"green\")\n",
    "# ax.fill_between(x, (group_2-group_2_ci), (group_2+group_2_ci), alpha=.1, color=\"goldenrod\")\n",
    "\n",
    "plt.xticks(x)\n",
    "plt.ylim(-10, 10)\n",
    "plt.axhline(y=0, color='black', linestyle='-.', linewidth=1)\n",
    "ax.legend()\n",
    "plt.xlabel(\"Discount Range\")\n",
    "plt.ylabel(\"Model Error [%]\")\n",
    "plt.savefig('../plots/final_errors.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
