{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['policy_nr_hashed', 'welcome_discount', 'last_data_year',\n",
      "       'first_data_year', 'churn', 'control_group', 'first_premium',\n",
      "       'last_premium', 'first_split', 'last_split', 'last_customer_age',\n",
      "       'last_accident_free_years', 'last_car_value', 'last_age_car',\n",
      "       'last_brand', 'last_type', 'last_weight', 'last_fuel_type',\n",
      "       'last_postcode', 'last_product', 'last_allrisk basis',\n",
      "       'last_allrisk compleet', 'last_allrisk royaal', 'last_wa-extra',\n",
      "       'last_wettelijke aansprakelijkheid', 'nr_cars', 'fake_alarm',\n",
      "       'policyholder_change', 'max_nr_coverages', 'last_nr_coverages',\n",
      "       'last_trend_nr_coverages', 'accident_years', 'last_year_car_change',\n",
      "       'last_change_premium_abs', 'last_change_premium_perc',\n",
      "       'years_since_last_car_change', 'n_last_vs_peak', 'last_vs_first_split',\n",
      "       'lpa', 'cum_change_premium_abs', 'cum_change_premium_perc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30151, 41)\n",
      "(19661, 41)\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming 'no WD and no LPA' represents untreated customers\n",
    "df = df[df[\"welcome_discount\"] == 1]\n",
    "\n",
    "\n",
    "# # Check if the DataFrame is not empty\n",
    "# if not untreated_df.empty:\n",
    "#     # Define your features and target variable\n",
    "#     X = untreated_df.drop(['policy_nr_hashed', 'last_data_year', 'churn', 'control_group', 'premiums', 'last_brand', 'last_type', 'last_fuel_type', 'last_postcode', 'last_product', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'years_since_last_car_change'], axis=1)\n",
    "#     y = untreated_df['churn']\n",
    "\n",
    "#     # Split the dataset into training and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# else:\n",
    "#     print(\"No untreated customers found. Check the filtering criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Features: ['churn', 'last_allrisk basis', 'last_allrisk compleet', 'last_allrisk royaal', 'last_wa-extra', 'fake_alarm', 'policyholder_change', 'n_last_vs_peak', 'lpa']\n",
      "Categorical Features: ['policy_nr_hashed', 'last_data_year', 'first_data_year', 'control_group', 'last_brand', 'last_type', 'last_fuel_type', 'last_product', 'nr_cars', 'max_nr_coverages', 'last_nr_coverages', 'last_trend_nr_coverages', 'last_year_car_change', 'years_since_last_car_change']\n",
      "Continuous Features: ['welcome_discount', 'first_premium', 'last_premium', 'first_split', 'last_split', 'last_customer_age', 'last_accident_free_years', 'last_car_value', 'last_age_car', 'last_weight', 'last_postcode', 'last_wettelijke aansprakelijkheid', 'accident_years', 'last_change_premium_abs', 'last_change_premium_perc', 'last_vs_first_split', 'cum_change_premium_abs', 'cum_change_premium_perc']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = []\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "\n",
    "# Define a threshold for the maximum number of unique values for a categorical column\n",
    "max_unique_values_for_categorical = 10\n",
    "\n",
    "# Iterate through each column to determine if it's categorical, continuous, or binary\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    if unique_values == 2:\n",
    "        # If exactly 2 unique values, treat column as binary\n",
    "        binary_features.append(column)\n",
    "    elif (df[column].dtype == 'object' or unique_values <= max_unique_values_for_categorical) and unique_values > 2:\n",
    "        # If object type or up to the threshold of unique values (and more than 2), treat as categorical\n",
    "        categorical_features.append(column)\n",
    "    else:\n",
    "        # Otherwise, treat as continuous\n",
    "        continuous_features.append(column)\n",
    "\n",
    "print(f'Binary Features: {binary_features}')\n",
    "print(f'Categorical Features: {categorical_features}')\n",
    "print(f'Continuous Features: {continuous_features}')\n",
    "\n",
    "for cat in categorical_features:\n",
    "     df[cat] = df[cat].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'binary_features' and 'continuous_features' are lists of your binary and continuous columns\n",
    "selected_features = binary_features + continuous_features\n",
    "\n",
    "# Select only the binary and continuous features, excluding the target variable 'churn'\n",
    "X = df[[col for col in df.columns if col in selected_features and col != \"churn\"]]\n",
    "y = df['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    # Add more parameters here if needed\n",
    "}\n",
    "\n",
    "# test\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV or BayesSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# bayes_search = BayesSearchCV(rf, search_spaces, n_iter=32, cv=5, n_jobs=-1) # Uncomment for BayesSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [07:18<00:00, 13.72s/trial, best loss: -0.6685793352283766]\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': 30, 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 175}\n"
     ]
    }
   ],
   "source": [
    "# # Define the search space\n",
    "# space = {\n",
    "#     'n_estimators': hp.choice('n_estimators', range(100, 301)),\n",
    "#     'max_depth': hp.choice('max_depth', [None, 5, 10, 15]),\n",
    "#     'min_samples_split': hp.choice('min_samples_split', range(2, 11))\n",
    "# }\n",
    "\n",
    "# # Define the objective function\n",
    "# def objective(params):\n",
    "#     rf = RandomForestClassifier(**params, random_state=42)\n",
    "#     best_score = cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "#     return {'loss': -best_score, 'status': STATUS_OK}\n",
    "\n",
    "# # Run the optimization with a different method for setting the random state\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=32, trials=trials, rstate=np.random.seed(42))\n",
    "\n",
    "# # Convert index values to actual values\n",
    "# best_params['n_estimators'] = range(100, 301)[best_params['n_estimators']]\n",
    "# best_params['max_depth'] = [None, 5, 10, 15][best_params['max_depth']]\n",
    "# best_params['min_samples_split'] = range(2, 11)[best_params['min_samples_split']]\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 301)),\n",
    "    'max_depth': hp.choice('max_depth', [None, 5, 10, 15]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 11)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 6)),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),  # Updated this line\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', [None, 10, 20, 30, 40, 50]),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    # 'class_weight': 'balanced' is set directly in the model initialization\n",
    "}\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Include class_weight='balanced' in the model\n",
    "    rf = RandomForestClassifier(**params, class_weight='balanced', random_state=42)\n",
    "    best_score = cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "    return {'loss': -best_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=32, trials=trials, rstate=np.random.seed(42))\n",
    "\n",
    "# Convert index values to actual values for the hyperparameters\n",
    "best_params['n_estimators'] = range(100, 301)[best_params['n_estimators']]\n",
    "best_params['max_depth'] = [None, 5, 10, 15, 20][best_params['max_depth']]\n",
    "best_params['min_samples_split'] = range(2, 11)[best_params['min_samples_split']]\n",
    "best_params['min_samples_leaf'] = range(1, 6)[best_params['min_samples_leaf']]\n",
    "best_params['max_features'] = ['sqrt', 'log2', None][best_params['max_features']]\n",
    "best_params['max_leaf_nodes'] = [None, 10, 20, 30, 40, 50][best_params['max_leaf_nodes']]\n",
    "best_params['criterion'] = ['gini', 'entropy'][best_params['criterion']]\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best score: 0.9563075467419552\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8622127457948353\n",
      "Test set score: 0.870218881273491\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'best_params' contains the best parameters found by Hyperopt\n",
    "# Create the RandomForestClassifier with the best parameters\n",
    "rf_best = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set (optional, for comparison)\n",
    "train_score = rf_best.score(X_train, y_train)\n",
    "print(\"Training set score:\", train_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = rf_best.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the best estimator to make predictions on the test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)  \u001b[38;5;66;03m# Ensure to use the transformed version of X_test if applicable\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the best estimator to make predictions on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)  # Ensure to use the transformed version of X_test if applicable\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate metrics with 'weighted' average for multiclass classification\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (Weighted): {precision}\")\n",
    "print(f\"Recall (Weighted): {recall}\")\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6467197830140702\n",
      "Precision (Weighted): 0.8982995867789405\n",
      "Recall (Weighted): 0.6467197830140702\n",
      "F1 Score (Weighted): 0.7300143465709406\n",
      "Confusion Matrix:\n",
      "[[3514 1940]\n",
      " [ 144  301]]\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters from Hyperopt and add class_weight='balanced'\n",
    "rf_best = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=245,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    max_leaf_nodes=30,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=8,\n",
    "    class_weight='balanced',  # Adjust class weights\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (Weighted): {precision}\")\n",
    "print(f\"Recall (Weighted): {recall}\")\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
