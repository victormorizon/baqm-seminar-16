{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, make_scorer, accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, brier_score_loss, log_loss, mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import auc as sklearn_auc\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['policy_nr_hashed', 'welcome_discount', 'last_data_year',\n",
      "       'first_data_year', 'churn', 'control_group', 'first_premium',\n",
      "       'last_premium', 'first_split', 'last_split', 'last_customer_age',\n",
      "       'last_accident_free_years', 'last_car_value', 'last_age_car',\n",
      "       'last_brand', 'last_type', 'last_weight', 'last_fuel_type',\n",
      "       'last_postcode', 'last_product', 'last_allrisk basis',\n",
      "       'last_allrisk compleet', 'last_allrisk royaal', 'last_wa-extra',\n",
      "       'nr_cars', 'fake_alarm', 'policyholder_change', 'max_nr_coverages',\n",
      "       'last_nr_coverages', 'last_trend_nr_coverages', 'accident_years',\n",
      "       'last_year_car_change', 'last_change_premium_abs',\n",
      "       'last_change_premium_perc', 'years_since_last_car_change',\n",
      "       'n_last_vs_peak', 'last_vs_first_split', 'lpa',\n",
      "       'cum_change_premium_abs', 'cum_change_premium_perc'],\n",
      "      dtype='object')\n",
      "Columns with missing values and their count:\n",
      "last_split                        10\n",
      "last_trend_nr_coverages        15030\n",
      "last_change_premium_abs        15030\n",
      "last_change_premium_perc       15030\n",
      "years_since_last_car_change    55600\n",
      "last_vs_first_split               10\n",
      "cum_change_premium_perc            7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "# Check for NaN values in each column\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Filter columns that have at least one missing value\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "print(\"Columns with missing values and their count:\")\n",
    "print(columns_with_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming 'no WD and no LPA' represents untreated customers\n",
    "df = df[df[\"welcome_discount\"] == 1]\n",
    "\n",
    "\n",
    "# # Check if the DataFrame is not empty\n",
    "# if not untreated_df.empty:\n",
    "#     # Define your features and target variable\n",
    "#     X = untreated_df.drop(['policy_nr_hashed', 'last_data_year', 'churn', 'control_group', 'premiums', 'last_brand', 'last_type', 'last_fuel_type', 'last_postcode', 'last_product', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'years_since_last_car_change'], axis=1)\n",
    "#     y = untreated_df['churn']\n",
    "\n",
    "#     # Split the dataset into training and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# else:\n",
    "#     print(\"No untreated customers found. Check the filtering criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Features: ['churn', 'last_allrisk basis', 'last_allrisk compleet', 'last_allrisk royaal', 'last_wa-extra', 'fake_alarm', 'policyholder_change', 'lpa']\n",
      "Categorical Features: ['policy_nr_hashed', 'last_data_year', 'control_group', 'last_brand', 'last_type', 'last_fuel_type', 'last_postcode', 'last_product', 'nr_cars', 'max_nr_coverages', 'last_nr_coverages', 'last_trend_nr_coverages', 'last_year_car_change', 'years_since_last_car_change', 'n_last_vs_peak']\n",
      "Continuous Features: ['welcome_discount', 'first_data_year', 'first_premium', 'last_premium', 'first_split', 'last_split', 'last_customer_age', 'last_accident_free_years', 'last_car_value', 'last_age_car', 'last_weight', 'accident_years', 'last_change_premium_abs', 'last_change_premium_perc', 'last_vs_first_split', 'cum_change_premium_abs', 'cum_change_premium_perc']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = []\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "\n",
    "# Define a threshold for the maximum number of unique values for a categorical column\n",
    "max_unique_values_for_categorical = 10\n",
    "\n",
    "# List the columns with fewer NaNs\n",
    "columns_with_fewer_nans = ['last_split', 'last_vs_first_split', 'cum_change_premium_perc']\n",
    "\n",
    "# Drop rows with NaNs in these specific columns\n",
    "df = df.dropna(subset=columns_with_fewer_nans)\n",
    "\n",
    "# Iterate through each column to determine if it's categorical, continuous, or binary\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    if unique_values == 2:\n",
    "        # If exactly 2 unique values, treat column as binary\n",
    "        binary_features.append(column)\n",
    "    elif (df[column].dtype == 'object' or unique_values <= max_unique_values_for_categorical) and unique_values > 2:\n",
    "        # If object type or up to the threshold of unique values (and more than 2), treat as categorical\n",
    "        categorical_features.append(column)\n",
    "    else:\n",
    "        # Otherwise, treat as continuous\n",
    "        continuous_features.append(column)\n",
    "\n",
    "print(f'Binary Features: {binary_features}')\n",
    "print(f'Categorical Features: {categorical_features}')\n",
    "print(f'Continuous Features: {continuous_features}')\n",
    "\n",
    "for cat in categorical_features:\n",
    "     df[cat] = df[cat].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to exclude from the features\n",
    "excluded_columns = ['churn', 'first_data_year', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'years_since_last_car_change']\n",
    "\n",
    "\n",
    "# Assuming 'binary_features' and 'continuous_features' are lists of your binary and continuous columns\n",
    "selected_features = binary_features + continuous_features\n",
    "\n",
    "# Select only the binary and continuous features, excluding the specified columns\n",
    "X = df[[col for col in df.columns if col in selected_features and col not in excluded_columns]]\n",
    "y = df['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    # Add more parameters here if needed\n",
    "}\n",
    "\n",
    "# test\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV or BayesSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# bayes_search = BayesSearchCV(rf, search_spaces, n_iter=32, cv=5, n_jobs=-1) # Uncomment for BayesSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [34:36<00:00, 64.89s/trial, best loss: 0.135595717721482]   \n",
      "Best parameters: {'max_depth': 15, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 253}\n"
     ]
    }
   ],
   "source": [
    "# # Define the search space\n",
    "# space = {\n",
    "#     'n_estimators': hp.choice('n_estimators', range(100, 301)),\n",
    "#     'max_depth': hp.choice('max_depth', [None, 5, 10, 15]),\n",
    "#     'min_samples_split': hp.choice('min_samples_split', range(2, 11))\n",
    "# }\n",
    "\n",
    "# # Define the objective function\n",
    "# def objective(params):\n",
    "#     rf = RandomForestClassifier(**params, random_state=42)\n",
    "#     best_score = cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "#     return {'loss': -best_score, 'status': STATUS_OK}\n",
    "\n",
    "# # Run the optimization with a different method for setting the random state\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=32, trials=trials, rstate=np.random.seed(42))\n",
    "\n",
    "# # Convert index values to actual values\n",
    "# best_params['n_estimators'] = range(100, 301)[best_params['n_estimators']]\n",
    "# best_params['max_depth'] = [None, 5, 10, 15][best_params['max_depth']]\n",
    "# best_params['min_samples_split'] = range(2, 11)[best_params['min_samples_split']]\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 301)),\n",
    "    'max_depth': hp.choice('max_depth', [None, 5, 10, 15]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 11)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 6)),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', [None, 10, 20, 30, 40, 50]),\n",
    "    # 'class_weight': 'balanced' is set directly in the model initialization\n",
    "}\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Include class_weight='balanced' in the model\n",
    "    rf = RandomForestClassifier(**params, class_weight='balanced', random_state=42)\n",
    "    best_score = cross_val_score(rf, X_train, y_train, scoring='neg_brier_score', cv=5).mean()\n",
    "    return {'loss': -best_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=32, trials=trials, rstate=np.random.seed(42))\n",
    "\n",
    "# Convert index values to actual values for the hyperparameters\n",
    "best_params['n_estimators'] = range(100, 301)[best_params['n_estimators']]\n",
    "best_params['max_depth'] = [None, 5, 10, 15, 20][best_params['max_depth']]\n",
    "best_params['min_samples_split'] = range(2, 11)[best_params['min_samples_split']]\n",
    "best_params['min_samples_leaf'] = range(1, 6)[best_params['min_samples_leaf']]\n",
    "best_params['max_leaf_nodes'] = [None, 10, 20, 30, 40, 50][best_params['max_leaf_nodes']]\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best score: 0.9563075467419552\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8622127457948353\n",
      "Test set score: 0.870218881273491\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'best_params' contains the best parameters found by Hyperopt\n",
    "# Create the RandomForestClassifier with the best parameters\n",
    "rf_best = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set (optional, for comparison)\n",
    "train_score = rf_best.score(X_train, y_train)\n",
    "print(\"Training set score:\", train_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = rf_best.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the best estimator to make predictions on the test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)  \u001b[38;5;66;03m# Ensure to use the transformed version of X_test if applicable\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the best estimator to make predictions on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)  # Ensure to use the transformed version of X_test if applicable\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate metrics with 'weighted' average for multiclass classification\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (Weighted): {precision}\")\n",
    "print(f\"Recall (Weighted): {recall}\")\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.7307589789122332\n",
      "Accuracy: 0.816658430297857\n",
      "Precision: [0.20975785 0.20976727 0.2097767  ... 1.         1.         1.        ]\n",
      "Recall: [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 4.28357250e-04\n",
      " 2.14178625e-04 0.00000000e+00]\n",
      "F1 Score: 0.5714585739787882\n",
      "Confusion Matrix:\n",
      "[[15457  2133]\n",
      " [ 1948  2721]]\n",
      "AUC-PR: 0.597293642196914\n",
      "Negative Brier Score: -0.13724210380549803\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters from Hyperopt and add class_weight='balanced'\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=253,\n",
    "    max_depth=15,\n",
    "    max_leaf_nodes=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=3,\n",
    "    class_weight='balanced',  # Adjust class weights\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "# Calculate metrics without 'weighted' average (using 'macro' average as an example)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "weights = {\n",
    "    'TP': 3.0,  # High importance to correctly identify churners\n",
    "    'TN': 1.0,  # Moderate importance for loyal customers\n",
    "    'FP': 2.0,  # Moderate to high cost of misclassifying loyal customers\n",
    "    'FN': 3.0   # High cost of missing actual churners\n",
    "}\n",
    "\n",
    "def weighted_confusion_matrix(y_true, y_pred, weights):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    weighted_tp = tp * weights['TP']\n",
    "    weighted_tn = tn * weights['TN']\n",
    "    weighted_fp = fp * weights['FP']\n",
    "    weighted_fn = fn * weights['FN']\n",
    "    return weighted_tp, weighted_tn, weighted_fp, weighted_fn\n",
    "\n",
    "# Function for Custom metric\n",
    "def custom_metric(y_true, y_pred, weights):\n",
    "    weighted_tp, weighted_tn, weighted_fp, weighted_fn = weighted_confusion_matrix(y_true, y_pred, weights)\n",
    "    return (weighted_tp + weighted_tn) / (weighted_tp + weighted_tn + weighted_fp + weighted_fn)\n",
    "\n",
    "\n",
    "# Calculate the AUC-PR\n",
    "y_scores = rf_best.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "auc_pr = sklearn_auc(recall, precision)\n",
    "\n",
    "# Calculate negative brier\n",
    "neg_brier_score = -brier_score_loss(y_test, y_scores)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'ROC AUC Score: {auc}')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"AUC-PR: {auc_pr}\")\n",
    "print(f\"Negative Brier Score: {neg_brier_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucakoster/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Brier Score: 0.13528077676681108\n",
      "Average Log Loss: 0.4291523318092195\n",
      "Average MAE: 0.2976445819165793\n",
      "Average MedAE: 0.2490841963427397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import brier_score_loss, log_loss, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define custom scoring functions\n",
    "def brier_score_loss_func(y_true, y_pred_probs):\n",
    "    return brier_score_loss(y_true, y_pred_probs[:, 1])\n",
    "\n",
    "def log_loss_func(y_true, y_pred_probs):\n",
    "    return log_loss(y_true, y_pred_probs[:, 1])\n",
    "\n",
    "# Convert custom scoring functions to scorers\n",
    "brier_scorer = make_scorer(brier_score_loss_func, needs_proba=True)\n",
    "log_loss_scorer = make_scorer(log_loss_func, needs_proba=True)\n",
    "\n",
    "# Initialize cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store scores\n",
    "brier_scores = []\n",
    "log_loss_scores = []\n",
    "mae_scores = []\n",
    "medae_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_idx, test_idx in cv.split(X_train, y_train):\n",
    "    # Split data using .iloc for Pandas DataFrame/Series\n",
    "    X_cv_train, X_cv_test = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "    y_cv_train, y_cv_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    # Fit model\n",
    "    rf_best.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_probs = rf_best.predict_proba(X_cv_test)\n",
    "\n",
    "    # Calculate scores\n",
    "    brier_scores.append(brier_score_loss(y_cv_test, y_pred_probs[:, 1]))\n",
    "    log_loss_scores.append(log_loss(y_cv_test, y_pred_probs))\n",
    "    mae_scores.append(mean_absolute_error(y_cv_test, y_pred_probs[:, 1]))\n",
    "    medae_scores.append(median_absolute_error(y_cv_test, y_pred_probs[:, 1]))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_brier = np.mean(brier_scores)\n",
    "avg_log_loss = np.mean(log_loss_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_medae = np.mean(medae_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average Brier Score: {avg_brier}\")\n",
    "print(f\"Average Log Loss: {avg_log_loss}\")\n",
    "print(f\"Average MAE: {avg_mae}\")\n",
    "print(f\"Average MedAE: {avg_medae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
