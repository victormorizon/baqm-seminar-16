{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prepped_data.csv\", index_col = 0, low_memory=False)\n",
    "df = df[df['first_data_year'] >= 2021]\n",
    "df = df.drop(columns=['policy_nr_hashed', 'last_data_year', 'first_data_year', 'control_group', 'churn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature type identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = []\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "\n",
    "# Define a threshold for the maximum number of unique values for a categorical column\n",
    "max_unique_values_for_categorical = 10\n",
    "\n",
    "# Iterate through each column to determine if it's categorical, continuous, or binary\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    if unique_values == 2:\n",
    "        # If exactly 2 unique values, treat column as binary\n",
    "        binary_features.append(column)\n",
    "    elif (df[column].dtype == 'object' or unique_values <= max_unique_values_for_categorical) and unique_values > 2:\n",
    "        # If object type or up to the threshold of unique values (and more than 2), treat as categorical\n",
    "        categorical_features.append(column)\n",
    "    else:\n",
    "        # Otherwise, treat as continuous\n",
    "        continuous_features.append(column)\n",
    "\n",
    "categorical_features.remove('years_since_last_car_change')\n",
    "print(f'Categorical Features: {categorical_features}')\n",
    "print(f'Continuous Features: {continuous_features}')\n",
    "continuous_features.append( 'years_since_last_car_change')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['welcome_discount']\n",
    "X = df.drop(columns=['welcome_discount'])\n",
    "\n",
    "\n",
    "for cat in categorical_features:\n",
    "     X[cat] = X[cat].astype(\"category\")\n",
    "\n",
    "\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming y_train is your training target variable\n",
    "# number_of_positive_instances = sum(y_train == 1)\n",
    "# number_of_negative_instances = sum(y_train == 0)\n",
    "\n",
    "# # Calculate the scale_pos_weight value\n",
    "# scale_pos_weight_value =  number_of_negative_instances /  number_of_positive_instances \n",
    "\n",
    "# print(f\"Suggested scale_pos_weight value: {scale_pos_weight_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters & Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 8, 20, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=int(space['n_estimators']),\n",
    "        max_depth=int(space['max_depth']),\n",
    "        learning_rate=space['learning_rate'],\n",
    "        subsample=space['subsample'],\n",
    "        gamma=space['gamma'],\n",
    "        colsample_bytree=space['colsample_bytree'],\n",
    "        min_child_weight=space['min_child_weight'],\n",
    "        reg_alpha=space['reg_alpha'],\n",
    "        reg_lambda=space['reg_lambda'],\n",
    "        objective='reg:squarederror',\n",
    "        enable_categorical = True,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    \n",
    "    # Using cross-validation for evaluation\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "    # The score is negative MSE, we return its negative value for minimization\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the algorithm\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best parameters:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics & Optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters auc: {'colsample_bytree': 0.729287347150897, 'gamma': 0.13865973186925346, 'learning_rate': 0.04065563148086127, 'max_depth': 4.0, 'min_child_weight': 9.0, 'n_estimators': 120.0, 'reg_alpha': 0.17293900654376804, 'reg_lambda': 0.43602874597504154, 'subsample': 0.9099295847018875}\n",
    "\n",
    "#Best parameters f1: {'colsample_bytree': 0.6363920729021093, 'gamma': 0.07599537381660419, 'learning_rate': 0.04269086278215556, 'max_depth': 9.0, 'min_child_weight': 10.0, 'n_estimators': 92.0, 'reg_alpha': 0.3268718059713048, 'reg_lambda': 0.19347992629476463, 'subsample': 0.9178288043568541}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, average_precision_score, make_scorer, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Use the best parameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    subsample=best['subsample'],\n",
    "    gamma=best['gamma'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    min_child_weight = best['min_child_weight'],\n",
    "    # scale_pos_weight=scale_pos_weight_value,\n",
    "    reg_alpha=best['reg_alpha'],\n",
    "    reg_lambda=best['reg_lambda'],\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    enable_categorical = True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "def mae_prob(y_true, y_pred_probs):\n",
    "    return mean_absolute_error(y_true, y_pred_probs)\n",
    "\n",
    "def medae_prob(y_true, y_pred_probs):\n",
    "    return median_absolute_error(y_true, y_pred_probs)\n",
    "\n",
    "mae_prob_scorer = make_scorer(mae_prob)\n",
    "medae_prob_scorer = make_scorer(medae_prob)\n",
    "\n",
    "# logloss_score = cross_val_score(model, X, y, scoring= 'neg_log_loss')\n",
    "# brier_score = cross_val_score(model, X, y, cv = 5, scoring = 'neg_brier_score')\n",
    "scores_mae = cross_val_score(model, X, y, cv=5, scoring=mae_prob_scorer)\n",
    "scores_medae = cross_val_score(model, X, y, cv=5, scoring=medae_prob_scorer)\n",
    "\n",
    "\n",
    "# print('CV Average logloss: {0:0.4f}'.format(np.mean(logloss_score)))\n",
    "print('CV mean_absolute_error: {0:0.4f}'.format(np.mean(scores_mae)))\n",
    "# print('CV Average  brier score: {0:0.4f}'.format(np.mean(brier_score)))\n",
    "print('CV median_absolute_error: {0:0.4f}'.format(np.mean(scores_medae)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
