{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, make_scorer, accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, brier_score_loss, log_loss, mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import auc as sklearn_auc\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "\n",
    "# Check for NaN values in each column\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Filter columns that have at least one missing value\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "print(\"Columns with missing values and their count:\")\n",
    "print(columns_with_missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/prepped_data.csv\", low_memory=False, index_col=0).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming 'no WD and no LPA' represents untreated customers\n",
    "#df = df[df[\"welcome_discount\"] == 1]\n",
    "\n",
    "\n",
    "# # Check if the DataFrame is not empty\n",
    "# if not untreated_df.empty:\n",
    "#     # Define your features and target variable\n",
    "#     X = untreated_df.drop(['policy_nr_hashed', 'last_data_year', 'churn', 'control_group', 'premiums', 'last_brand', 'last_type', 'last_fuel_type', 'last_postcode', 'last_product', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'years_since_last_car_change'], axis=1)\n",
    "#     y = untreated_df['churn']\n",
    "\n",
    "#     # Split the dataset into training and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# else:\n",
    "#     print(\"No untreated customers found. Check the filtering criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ategorical_features = []\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "\n",
    "# Define a threshold for the maximum number of unique values for a categorical column\n",
    "max_unique_values_for_categorical = 10\n",
    "\n",
    "# List the columns with fewer NaNs\n",
    "#columns_with_fewer_nans = ['last_split', 'last_vs_first_split', 'cum_change_premium_perc']\n",
    "\n",
    "# Drop rows with NaNs in these specific columns\n",
    "#df = df.dropna(subset=columns_with_fewer_nans)\n",
    "\n",
    "# Iterate through each column to determine if it's categorical, continuous, or binary\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    if unique_values == 2:\n",
    "        # If exactly 2 unique values, treat column as binary\n",
    "        binary_features.append(column)\n",
    "    elif (df[column].dtype == 'object' or unique_values <= max_unique_values_for_categorical) and unique_values > 2:\n",
    "        # If object type or up to the threshold of unique values (and more than 2), treat as categorical\n",
    "        categorical_features.append(column)\n",
    "    else:\n",
    "        # Otherwise, treat as continuous\n",
    "        continuous_features.append(column)\n",
    "\n",
    "print(f'Binary Features: {binary_features}')\n",
    "print(f'Categorical Features: {categorical_features}')\n",
    "print(f'Continuous Features: {continuous_features}')\n",
    "\n",
    "for cat in categorical_features:\n",
    "     df[cat] = df[cat].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.get_dummies(df, columns=categorical_features)\n",
    "# List of columns to exclude from the features\n",
    "excluded_columns = ['churn', 'welcome_discount', 'last_trend_nr_coverages', 'last_change_premium_abs', 'last_change_premium_perc', 'years_since_last_car_change' ]\n",
    "\n",
    "# Assuming 'binary_features' and 'continuous_features' are lists of your binary and continuous columns\n",
    "selected_features = binary_features + continuous_features\n",
    "\n",
    "# Select only the binary and continuous features, excluding the specified columns\n",
    "X = df[[col for col in df.columns if col in selected_features and col not in excluded_columns]]\n",
    "y = df['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    # Add more parameters here if needed\n",
    "}\n",
    "\n",
    "# test\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV or BayesSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# bayes_search = BayesSearchCV(rf, search_spaces, n_iter=32, cv=5, n_jobs=-1) # Uncomment for BayesSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 301)),\n",
    "    'max_depth': hp.choice('max_depth', [None, 5, 10, 15]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 11)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 6)),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', [None, 10, 20, 30, 40, 50]),\n",
    "    # 'class_weight': 'balanced' is set directly in the model initialization\n",
    "}\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Include class_weight='balanced' in the model\n",
    "    rf = RandomForestClassifier(**params, class_weight='balanced', random_state=42)\n",
    "    best_score = cross_val_score(rf, X_train, y_train, scoring='neg_brier_score', cv=5).mean()\n",
    "    return {'loss': -best_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=32, trials=trials, rstate=np.random.seed(42))\n",
    "\n",
    "# Convert index values to actual values for the hyperparameters\n",
    "best_params['n_estimators'] = range(100, 301)[best_params['n_estimators']]\n",
    "best_params['max_depth'] = [None, 5, 10, 15, 20][best_params['max_depth']]\n",
    "best_params['min_samples_split'] = range(2, 11)[best_params['min_samples_split']]\n",
    "best_params['min_samples_leaf'] = range(1, 6)[best_params['min_samples_leaf']]\n",
    "best_params['max_leaf_nodes'] = [None, 10, 20, 30, 40, 50][best_params['max_leaf_nodes']]\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'best_params' contains the best parameters found by Hyperopt\n",
    "# Create the RandomForestClassifier with the best parameters\n",
    "rf_best = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set (optional, for comparison)\n",
    "train_score = rf_best.score(X_train, y_train)\n",
    "print(\"Training set score:\", train_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = rf_best.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimator to make predictions on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)  # Ensure to use the transformed version of X_test if applicable\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate metrics with 'weighted' average for multiclass classification\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (Weighted): {precision}\")\n",
    "print(f\"Recall (Weighted): {recall}\")\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters from Hyperopt and add class_weight='balanced'\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=286,\n",
    "    max_depth=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',  # Adjust class weights\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Print the metrics\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision (Weighted): {precision}\")\n",
    "# print(f\"Recall (Weighted): {recall}\")\n",
    "# print(f\"F1 Score (Weighted): {f1}\")\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Calculate metrics without 'weighted' average (using 'macro' average as an example)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate the AUC-PR\n",
    "y_scores = rf_best.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "auc_pr = sklearn_auc(recall, precision)\n",
    "\n",
    "# Calculate negative brier\n",
    "neg_brier_score = -brier_score_loss(y_test, y_scores)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'ROC AUC Score: {auc}')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"AUC-PR: {auc_pr}\")\n",
    "print(f\"Negative Brier Score: {neg_brier_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import brier_score_loss, log_loss, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define custom scoring functions\n",
    "def brier_score_loss_func(y_true, y_pred_probs):\n",
    "    return brier_score_loss(y_true, y_pred_probs[:, 1])\n",
    "\n",
    "def log_loss_func(y_true, y_pred_probs):\n",
    "    return log_loss(y_true, y_pred_probs[:, 1])\n",
    "\n",
    "# Convert custom scoring functions to scorers\n",
    "brier_scorer = make_scorer(brier_score_loss_func, needs_proba=True)\n",
    "log_loss_scorer = make_scorer(log_loss_func, needs_proba=True)\n",
    "\n",
    "# Initialize cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store scores\n",
    "brier_scores = []\n",
    "log_loss_scores = []\n",
    "mae_scores = []\n",
    "medae_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_idx, test_idx in cv.split(X_train, y_train):\n",
    "    # Split data using .iloc for Pandas DataFrame/Series\n",
    "    X_cv_train, X_cv_test = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "    y_cv_train, y_cv_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "    # Fit model\n",
    "    rf_best.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_probs = rf_best.predict_proba(X_cv_test)\n",
    "\n",
    "    # Calculate scores\n",
    "    brier_scores.append(brier_score_loss(y_cv_test, y_pred_probs[:, 1]))\n",
    "    log_loss_scores.append(log_loss(y_cv_test, y_pred_probs))\n",
    "    mae_scores.append(mean_absolute_error(y_cv_test, y_pred_probs[:, 1]))\n",
    "    medae_scores.append(median_absolute_error(y_cv_test, y_pred_probs[:, 1]))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_brier = np.mean(brier_scores)\n",
    "avg_log_loss = np.mean(log_loss_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_medae = np.mean(medae_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average Brier Score: {avg_brier}\")\n",
    "print(f\"Average Log Loss: {avg_log_loss}\")\n",
    "print(f\"Average MAE: {avg_mae}\")\n",
    "print(f\"Average MedAE: {avg_medae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
